{
  "label": "import torch\nimport math\nfrom functools import partial\nfrom typing import Tuple\n\ntry:\n    # fa3\n    from flash_attn_interface import flash_attn_func \nexcept:\n    # fa2\n    from flash_attn import flash_attn_func\n\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _fused_attn_fwd_kernel(P,\n                           W,\n                           O,\n                           N:tl.constexpr,\n                           D:tl.constexpr,\n                           BLOCK_N:tl.constexpr=32,\n                           CHUNK_N:tl.constexpr=1024\n                           ):\n    start_n = tl.cast(tl.program_id(0), tl.int64) * CHUNK_N + tl.program_id(1) * BLOCK_N\n    if start_n >= N:\n        return\n    off_n = start_n + tl.arange(0, BLOCK_N)\n    mask = off_n < N\n\n    acc = tl.zeros((BLOCK_N, D), dtype=tl.float32)\n    offset = off_n[:, None] * D + tl.arange(0, D)[None, :]\n    for i in range(3):\n        p = tl.load(P+i).to(tl.pointer_type(O.dtype.element_ty))\n        o = tl.load(p + offset, mask=mask[:, None], other=0.).to(tl.float32)\n        w = tl.load(W + off_n * 3 + i, mask=mask, other=0.).to(tl.float32)\n        acc += o * tl.sigmoid(w)[:, None]\n    tl.store(O + offset, acc, mask=mask[:, None])\n\n\n@triton.jit\ndef _fused_attn_bwd_kernel(DP,\n                           DW,\n                           DO,\n                           P,\n                           W,\n                           N:tl.constexpr,\n                           D:tl.constexpr,\n                           BLOCK_N:tl.constexpr=32,\n                           CHUNK_N:tl.constexpr=1024\n                           ):\n    start_n = tl.cast(tl.program_id(0), tl.int64) * CHUNK_N + tl.program_id(1) * BLOCK_N\n    if start_n >= N:\n        return\n    off_n = start_n + tl.arange(0, BLOCK_N)\n    mask = off_n < N\n\n    offset = off_n[:, None] * D + tl.arange(0, D)[None, :]\n    dcombine_o = tl.load(DO + offset, mask=mask[:, None], other=0.).to(tl.float32)\n\n    i = tl.program_id(2)\n    p = tl.load(P+i).to(tl.pointer_type(DO.dtype.element_ty))\n    dp = tl.load(DP+i).to(tl.pointer_type(DO.dtype.element_ty))\n    o = tl.load(p + offset, mask=mask[:, None], other=0.).to(tl.float32)\n    w = tl.load(W + off_n * 3 + i, mask=mask, other=0.).to(tl.float32)\n    sigmoid_w = tl.sigmoid(w)\n    do = dcombine_o * sigmoid_w[:, None]\n    dsigmoid_w = tl.sum(dcombine_o * o, -1)\n    dw = sigmoid_w * (1 - sigmoid_w) * dsigmoid_w\n    tl.store(dp + offset, do, mask=mask[:, None])\n    tl.store(DW + off_n * 3 + i, dw, mask=mask)\n\n    \n\nclass _FusedAttention(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, a, b, c, w):\n        assert a.is_contiguous() and b.is_contiguous() and c.is_contiguous() and w.is_contiguous()\n        B, S, H, D = a.shape\n        assert w.size(-1) == 3\n        assert math.log2(D).is_integer()\n        o = torch.empty_like(a)\n        N = B * S * H\n        p = torch.tensor([a.data_ptr(), b.data_ptr(), c.data_ptr()], dtype=torch.int64, device=a.device)\n        kwargs = {'BLOCK_N':16, \"num_warps\":8, \"num_stages\":2}\n        grid = lambda meta: (triton.cdiv(N, meta['CHUNK_N']), triton.cdiv(meta['CHUNK_N'], meta['BLOCK_N']))\n        _fused_attn_fwd_kernel[grid](p, \n                                     w, \n                                     o, \n                                     N, \n                                     D,\n                                     **kwargs\n                                     )\n        ctx.save_for_backward(a, b, c, w)\n        ctx.p = p\n        ctx.N = N\n        ctx.D = D\n\n        return o \n    \n    @staticmethod\n    def backward(ctx, do):\n        assert do.is_contiguous()\n        a, b, c, w = ctx.saved_tensors\n        da = torch.empty_like(a)\n        db = torch.empty_like(b)\n        dc = torch.empty_like(c)\n        dw = torch.empty_like(w)\n        dp = torch.tensor([da.data_ptr(), db.data_ptr(), dc.data_ptr()], dtype=torch.int64, device=a.device)\n        kwargs = {'BLOCK_N':4, \"num_warps\":1, \"num_stages\":4}\n        grid = lambda meta: (triton.cdiv(ctx.N, meta['CHUNK_N']), triton.cdiv(meta['CHUNK_N'], meta['BLOCK_N']), 3)\n        _fused_attn_bwd_kernel[grid](dp, \n                                     dw,\n                                     do,\n                                     ctx.p,\n                                     w, \n                                     ctx.N, \n                                     ctx.D,\n                                     **kwargs\n                                     )\n        return da, db, dc, dw\n\n\ndef fused_attention(a, b, c, w):\n    return _FusedAttention.apply(a, b, c, w)\n\nimport torch\nimport math\nfrom functools import partial\nfrom typing import Tuple\n\ntry:\n    # fa3\n    from flash_attn_interface import flash_attn_func \nexcept:\n    # fa2\n    from flash_attn import flash_attn_func\n\n# [2022-10-23] Downloaded from https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py\n# for benchmarking.\n# We fixed a few dtype cast to make it work for bf16\n\n\"\"\"\nFused Attention\n===============\nThis is a Triton implementation of the Flash Attention algorithm\n(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n\"\"\"\n\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n# @triton.autotune([triton.Config({}, num_stages=ns, num_warps=nw)\n#                   for ns in [1,2,4]\n#                   for nw in [4, 8]\n#                   ],\n#                   key=['D1','D2','BLOCK_SIZE_N'])\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=2),\n        triton.Config({}, num_stages=1, num_warps=4),\n        triton.Config({}, num_stages=2, num_warps=4),\n    ],\n    key=['D1','D2','BLOCK_SIZE_N']\n)\n@triton.jit\ndef _block_compress_fwd(X, W, PE, Y, \n                        x_stride_b, x_stride_n, x_stride_h, x_stride_d,\n                        y_stride_b, y_stride_m, y_stride_h, y_stride_d,\n                        stride, kernel_size, \n                        D,\n                        D1:tl.constexpr, D2:tl.constexpr, BLOCK_SIZE_N:tl.constexpr):\n    off_b = tl.cast(tl.program_id(0), tl.int64)\n    off_h = tl.cast(tl.program_id(1), tl.int64)\n    off_m = tl.cast(tl.program_id(2), tl.int64)\n    \n    X += off_b * x_stride_b + off_h * x_stride_h + stride * off_m * x_stride_n\n    Y += off_b * y_stride_b + off_h * y_stride_h + off_m * y_stride_m\n\n    rows = tl.arange(0, BLOCK_SIZE_N)\n    mask = rows < kernel_size\n\n    w = tl.load(W + rows, mask=mask, other=0.).to(tl.float32)\n\n    x_ptrs = X + rows[:, None] * x_stride_n + tl.arange(0, D1)[None, :]\n    x = tl.load(x_ptrs, mask=mask[:, None], other=0.).to(tl.float32)\n    pe_ptrs = PE + rows[:, None] * D + tl.arange(0, D1)[None, :]\n    pe = tl.load(pe_ptrs, mask=mask[:, None], other=0.).to(tl.float32)\n    y = tl.sum((x + pe) * w[:, None], axis=0) / kernel_size\n    y_ptrs = Y + tl.arange(0, D1)\n    tl.store(y_ptrs, y)\n    \n    if D2 > 0:\n        x_ptrs = X + rows[:, None] * x_stride_n + tl.arange(0, D2)[None, :] + D1\n        x = tl.load(x_ptrs, mask=mask[:, None], other=0.).to(tl.float32)\n        pe_ptrs = PE + rows[:, None] * D + tl.arange(0, D2)[None, :] + D1\n        pe = tl.load(pe_ptrs, mask=mask[:, None], other=0.).to(tl.float32)\n        y = tl.sum((x + pe) * w[:, None], axis=0) / kernel_size\n        y_ptrs = Y + tl.arange(0, D2) + D1\n        tl.store(y_ptrs, y)\n\n# @triton.autotune([triton.Config({}, num_stages=ns, num_warps=nw)\n#                   for ns in [1,2,4]\n#                   for nw in [4, 8]\n#                   ],\n#                   key=['D1','D2', 'BLOCK_SIZE_N'])\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=4),\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=4),\n    ],\n    key=['D1','D2', 'BLOCK_SIZE_N']\n)\n@triton.jit\ndef _block_compress_dwdpe(DY, DW, DPE,\n                          X, W, PE,\n                          dy_stride_b, dy_stride_m, dy_stride_h, dy_stride_d,\n                          x_stride_b, x_stride_n, x_stride_h, x_stride_d,\n                          stride, kernel_size, num_blocks, NUM_SMS, \n                          B, H, D,\n                          D1:tl.constexpr, D2:tl.constexpr, BLOCK_SIZE_N:tl.constexpr\n                          ):\n    pid = tl.cast(tl.program_id(0), tl.int64)\n    current_id = pid\n    total = B * H * num_blocks\n\n    rows = tl.arange(0, BLOCK_SIZE_N)\n    mask = rows < kernel_size\n    cols = tl.arange(0, D1)\n\n    w = tl.load(W+rows, mask=mask, other=0.).to(tl.float32)\n    pe_ptrs = PE + rows[:, None] * D + cols[None, :]\n    pe = tl.load(pe_ptrs, mask=mask[:, None], other=0.).to(tl.float32)\n\n    dpe = tl.zeros((BLOCK_SIZE_N, D1), dtype=tl.float32)\n    dw = tl.zeros((BLOCK_SIZE_N, ), dtype=tl.float32)\n    if D2 > 0:\n        cols2 = tl.arange(0, D2) + D1\n        pe_ptrs2 = PE + rows[:, None] * D + cols2[None, :]\n        pe2 = tl.load(pe_ptrs2, mask=mask[:, None], other=0.).to(tl.float32)\n        dpe2 = tl.zeros((BLOCK_SIZE_N, D2), dtype=tl.float32)\n\n    while current_id < total:\n        off_m = current_id % num_blocks\n        off_bh = current_id // num_blocks\n        off_b = off_bh // H\n        off_h = off_bh % H\n\n        dy_ptrs = DY + off_b * dy_stride_b + off_h * dy_stride_h + off_m * dy_stride_m + cols\n        x_ptrs = X + off_b * x_stride_b + off_h * x_stride_h \\\n                + (stride * off_m + rows[:, None]) * x_stride_n \\\n                + cols[None, :]\n        dy = tl.load(dy_ptrs).to(tl.float32)\n        x = tl.load(x_ptrs, mask=mask[:, None], other=0.).to(tl.float32)\n        x_pe = x + pe\n        dw += tl.sum(x_pe * dy[None, :], axis=1)\n        dpe += w[:, None] * dy[None, :]\n\n        if D2 > 0:\n            dy_ptrs2 = DY + off_b * dy_stride_b + off_h * dy_stride_h + off_m * dy_stride_m + cols2\n            x_ptrs2 = X + off_b * x_stride_b + off_h * x_stride_h \\\n                    + (stride * off_m + rows[:, None]) * x_stride_n \\\n                    + cols2[None, :]\n            dy2 = tl.load(dy_ptrs2).to(tl.float32)\n            x2 = tl.load(x_ptrs2, mask=mask[:, None], other=0.).to(tl.float32)\n            x_pe2 = x2 + pe2\n            dw += tl.sum(x_pe2 * dy2[None, :], axis=1)\n            dpe2 += w[:, None] * dy2[None, :]\n\n        current_id += NUM_SMS\n\n    dw_ptrs = DW + pid * kernel_size + rows\n    dpe_ptrs = DPE + pid * kernel_size * D + rows[:, None] * D + cols[None, :]\n    tl.store(dw_ptrs, dw / kernel_size)\n    tl.store(dpe_ptrs, dpe / kernel_size, mask=mask[:, None])\n    if D2 > 0:\n        dpe_ptrs2 = DPE + pid * kernel_size * D + rows[:, None] * D + cols2[None, :]\n        tl.store(dpe_ptrs2, dpe2 / kernel_size, mask=mask[:, None])\n\n\n# @triton.autotune([triton.Config({}, num_stages=ns, num_warps=nw)\n#                   for ns in [1,2,4]\n#                   for nw in [4, 8]\n#                   ],\n#                   key=['D1', 'D2'])\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=2),\n        triton.Config({}, num_stages=1, num_warps=4),\n        triton.Config({}, num_stages=2, num_warps=4),\n    ],\n    key=['D1', 'D2']\n)\n@triton.jit\ndef _block_compress_dx(DY, DX,\n                        W, \n                        dy_stride_b, dy_stride_m, dy_stride_h, dy_stride_d,\n                        dx_stride_b, dx_stride_n, dx_stride_h, dx_stride_d,\n                        stride:tl.constexpr, kernel_size, num_blocks, \n                        D1:tl.constexpr, D2:tl.constexpr,\n                          ):\n    off_b = tl.cast(tl.program_id(0), tl.int64)\n    off_h = tl.cast(tl.program_id(1), tl.int64)\n    pid_k = tl.cast(tl.program_id(2), tl.int64)\n\n    DY += off_b * dy_stride_b + off_h * dy_stride_h\n    DX += off_b * dx_stride_b + off_h * dx_stride_h\n\n    rows = tl.arange(0, stride)\n    cols = tl.arange(0, D1)\n    # tl.static_print()\n    dx = tl.zeros((stride, D1), dtype=tl.float32)\n    if D2>0:\n        cols2 = tl.arange(0, D2) + D1\n        dx2 = tl.zeros((stride, D2), dtype=tl.float32)\n    for idx in range(0, (kernel_size/stride).to(tl.int32)):\n        block_idx = pid_k - idx\n        if block_idx >=0 and block_idx < num_blocks:\n            dy_ptrs = DY + block_idx * dy_stride_m + cols\n            dy = tl.load(dy_ptrs).to(tl.float32)\n            w = tl.load(W + idx*stride + rows).to(tl.float32)\n            dx += dy[None, :] * w[:, None]\n            if D2 > 0:\n                dy_ptrs2 = DY + block_idx * dy_stride_m + cols2\n                dy2 = tl.load(dy_ptrs2).to(tl.float32)\n                dx2 += dy2[None, :] * w[:, None]\n    dx_ptrs = DX + (pid_k * stride + rows[:, None]) * dx_stride_n + cols[None, :]\n    tl.store(dx_ptrs, dx / kernel_size)\n    if D2 > 0:\n        dx_ptrs2 = DX + (pid_k * stride + rows[:, None]) * dx_stride_n + cols2[None, :]\n        tl.store(dx_ptrs2, dx2 / kernel_size)\n\n\n\nclass _BlockCompress(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, pe, stride):\n        B, N, H, D = x.shape\n        kernel_size = len(weight)\n        assert kernel_size % stride == 0\n        assert math.log2(kernel_size).is_integer()\n        assert N >= kernel_size\n        num_blocks = (N - kernel_size) // stride + 1\n        assert num_blocks > 0\n\n        BLOCK_SIZE_N = triton.next_power_of_2(kernel_size)\n        \n        if math.log2(D).is_integer():\n            D1 = D\n            D2 = 0\n        else:\n            D1 = 2**int(math.log2(D-1))\n            D2 = D - D1\n            assert math.log2(D2).is_integer()\n        y = torch.empty(B, num_blocks, H, D, device=x.device, dtype=x.dtype)\n        grids = (B, H, num_blocks)\n        # Remove hardcoded kwargs\n        _block_compress_fwd[grids](x, weight, pe, y,\n                                   *x.stride(),\n                                   *y.stride(),\n                                    stride, kernel_size,\n                                    D, D1, D2, BLOCK_SIZE_N\n                                   )\n        ctx.save_for_backward(x, weight, pe)\n        ctx.infos = (B, H, N, D, kernel_size, stride, num_blocks, D1, D2, BLOCK_SIZE_N)\n        return y\n    \n    @staticmethod\n    def backward(ctx, dy):\n        x, weight, pe = ctx.saved_tensors\n        B, H, N, D, kernel_size, stride, num_blocks, D1, D2, BLOCK_SIZE_N = ctx.infos\n\n        NUM_SMS = torch.cuda.get_device_properties('cuda').multi_processor_count\n        dw = torch.empty(NUM_SMS, kernel_size, device=x.device, dtype=torch.float32)\n        dpe = torch.empty(NUM_SMS, kernel_size, D, device=x.device, dtype=torch.float32)\n        # Remove hardcoded kwargs\n        _block_compress_dwdpe[(NUM_SMS,)](dy, dw, dpe,\n                                         x, weight, pe,\n                                         *dy.stride(),\n                                         *x.stride(),\n                                         stride, kernel_size, num_blocks, NUM_SMS,\n                                         B, H, D, \n                                         D1, D2, BLOCK_SIZE_N\n                                         )\n        dw = dw.sum(0).to(weight.dtype)\n        dpe = dpe.sum(0).to(pe.dtype)\n\n        K = (stride * num_blocks + (kernel_size - stride)) // stride\n        dx = torch.empty_like(x)\n        dx[:, :, num_blocks * stride + kernel_size - stride:] = 0\n        # Remove hardcoded kwargs\n        _block_compress_dx[(B,H,K)](dy, dx,\n                                         weight,\n                                         *dy.stride(),\n                                         *dx.stride(),\n                                         stride, kernel_size, num_blocks, \n                                         D1, D2\n                                         )\n        return dx, dw, dpe, None\n\n\n\ndef blcok_compress(x, weight, pe, stride):\n    return _BlockCompress.apply(x, weight, pe, stride)\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N': bsn, 'BLOCK_SIZE_M': bsm}, num_stages=ns, num_warps=nw)\n#                  for bsm in [32, 64]\n#                  for bsn in [64, 128]\n#                  for ns in [1, 2, 3, 4, 5]\n#                  for nw in [2, 4, 8]\n#                  ], key=['N', \"M\"])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_M': 16}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_M': 16}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_M': 32}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_M': 32}, num_stages=2, num_warps=4),\n    ],\n    key=['N', 'M', 'D1', 'D2', 'VD']\n)\n@triton.jit\ndef _comp_attn_fwd_kernel(Q, K, V, O, LSE, \n                q_stride_b, q_stride_n, q_stride_h, q_stride_d,\n                k_stride_b, k_stride_m, k_stride_h, k_stride_d,\n                v_stride_b, v_stride_m, v_stride_h, v_stride_d,\n                o_stride_b, o_stride_n, o_stride_h, o_stride_d,\n                lse_stride_b, lse_stride_h, lse_stride_n,\n                sm_scale, kernel_size, stride,\n                B, N, M, QH, KH, \n                D1: tl.constexpr, D2: tl.constexpr, VD: tl.constexpr, \n                BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):\n\n    off_b = tl.cast(tl.program_id(0), tl.int64)\n    off_qh = tl.cast(tl.program_id(1), tl.int64)\n    start_n = tl.cast(tl.program_id(2), tl.int64) * BLOCK_SIZE_N\n    off_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    off_kh = off_qh // (QH // KH)\n\n    Q += off_b * q_stride_b + off_qh * q_stride_h\n    K += off_b * k_stride_b + off_kh * k_stride_h \n    V += off_b * v_stride_b + off_kh * v_stride_h\n    O += off_b * o_stride_b + off_qh * o_stride_h\n    LSE += off_b * lse_stride_b + off_qh * lse_stride_h\n\n    q = tl.load(Q + off_n[:, None] * q_stride_n + tl.arange(0, D1)[None, :], mask=off_n[:, None] < N, other=0.)\n    if D2 > 0:\n        q2 = tl.load(Q + off_n[:, None] * q_stride_n + tl.arange(0, D2)[None, :] + D1, mask=off_n[:, None] < N, other=0.)\n\n    m_i = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_SIZE_N, VD], dtype=tl.float32)\n\n    block_idx = tl.arange(0, BLOCK_SIZE_M)\n    for start_kv_idx in range(kernel_size-1, start_n + BLOCK_SIZE_N, BLOCK_SIZE_M * stride):\n        k = tl.load(K + block_idx[None, :] * k_stride_m + tl.arange(0, D1)[:, None], mask=block_idx[None, :] < M, other=0.)\n        attn_score = tl.dot(q, k)\n        if D2>0:\n            k2 = tl.load(K + block_idx[None, :] * k_stride_m + tl.arange(0, D2)[:, None] + D1, mask=block_idx[None, :] < M, other=0.)\n            attn_score = tl.dot(q2, k2, attn_score)\n\n        k_idx = block_idx * stride + kernel_size - 1\n        attn_score = tl.where(off_n[:, None] >= k_idx[None, :], attn_score * sm_scale, float('-inf'))\n\n        m_ij = tl.max(attn_score, axis=1)\n        new_m_i = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - new_m_i)\n\n        exp_attn_score = tl.exp(attn_score - new_m_i[:, None])\n\n        l_i = l_i * alpha + tl.sum(exp_attn_score, axis=-1)\n        acc = acc * alpha[:, None]\n\n        v = tl.load(V + block_idx[:, None] * v_stride_m + tl.arange(0, VD)[None, :], mask=block_idx[:, None] < M, other=0.)\n        acc = tl.dot(exp_attn_score.to(v.dtype), v, acc=acc)\n\n        m_i = new_m_i\n        block_idx += BLOCK_SIZE_M\n\n    acc /= l_i[:, None]\n    lse = m_i + tl.log(l_i)\n    if start_n == 0:\n        acc = tl.where(off_n[:, None]>=(kernel_size-1), acc, 0)\n        lse = tl.where(off_n>=(kernel_size-1), lse, 0)\n    tl.store(O + off_n[:, None] * o_stride_n + tl.arange(0, VD)[None, :], acc, mask=off_n[:, None] < N)   \n    tl.store(LSE + off_n * lse_stride_n, lse, mask=off_n < N)\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N': bsn}, num_stages=ns, num_warps=nw)\n#                  for bsn in [16, 32, 64, 128]\n#                  for ns in [1, 2, 4]\n#                  for nw in [4, 8]\n#                  ], key=['N'])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE_N': 128}, num_stages=1, num_warps=8),\n    ],\n    key=['N']\n)\n@triton.jit\ndef _comp_attn_bwd_prepro(O,DO,Delta,\n                    o_stride_b, o_stride_n, o_stride_h, o_stride_d,\n                    delta_stride_b, delta_stride_h, delta_stride_n,\n                    N, VD: tl.constexpr,\n                    BLOCK_SIZE_N: tl.constexpr\n                    ):\n    off_b = tl.cast(tl.program_id(0), tl.int64)\n    off_h = tl.cast(tl.program_id(1), tl.int64)\n    off_n = tl.cast(tl.program_id(2), tl.int64) * BLOCK_SIZE_N\n\n    O += off_b * o_stride_b + off_h * o_stride_h\n    DO += off_b * o_stride_b + off_h * o_stride_h\n    Delta += off_b * delta_stride_b + off_h * delta_stride_h\n    \n    rows = tl.arange(0, BLOCK_SIZE_N) + off_n\n    row_mask = rows < N\n    cols = tl.arange(0, VD)\n    \n    o = tl.load(O + rows[:, None] * o_stride_n + cols[None, :], mask=row_mask[:, None], other=0.).to(tl.float32)\n    do = tl.load(DO + rows[:, None] * o_stride_n + cols[None, :], mask=row_mask[:, None], other=0.).to(tl.float32)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + rows, delta, mask=row_mask)\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N': bsn, 'BLOCK_SIZE_M': bsm}, num_stages=ns, num_warps=nw)\n#                  for bsm in [32, 64, 128]\n#                  for bsn in [32, 64, 128]\n#                  for ns in [1, 2]\n#                  for nw in [4, 8]\n#                  ], key=['N', \"M\"])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_M': 16}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_M': 16}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_M': 32}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_M': 32}, num_stages=2, num_warps=4),\n    ],\n    key=['N', 'M', 'D1', 'D2', 'VD']\n)\n@triton.jit\ndef _comp_attn_dkv_kernel(DK, DV, DO, \n                Q, K, V, \n                Lse, Delta,\n                q_stride_b, q_stride_n, q_stride_h, q_stride_d,\n                k_stride_b, k_stride_m, k_stride_h, k_stride_d,\n                v_stride_b, v_stride_m, v_stride_h, v_stride_d,\n                dk_stride_b, dk_stride_m, dk_stride_h, dk_stride_d,\n                dv_stride_b, dv_stride_m, dv_stride_h, dv_stride_d,\n                do_stride_b, do_stride_n, do_stride_h, do_stride_d,\n                lse_stride_b, lse_stride_h, lse_stride_n,\n                sm_scale, kernel_size, stride,\n                B, N, M, QH, KH, \n                D1: tl.constexpr, D2: tl.constexpr, VD: tl.constexpr, \n                BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_M: tl.constexpr\n                ):\n    start_m = tl.cast(tl.program_id(0), tl.int64) * BLOCK_SIZE_M\n    off_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    off_b = tl.cast(tl.program_id(1), tl.int64)\n    off_qh = tl.cast(tl.program_id(2), tl.int64)\n    \n    off_kh = off_qh // (QH // KH)\n\n    Q += off_b * q_stride_b + off_qh * q_stride_h\n    K += off_b * k_stride_b + off_kh * k_stride_h \n    V += off_b * v_stride_b + off_kh * v_stride_h\n    DK += off_b * dk_stride_b + off_qh * dk_stride_h \n    DV += off_b * dv_stride_b + off_qh * dv_stride_h\n    DO += off_b * do_stride_b + off_qh * do_stride_h\n    Lse += off_b * lse_stride_b + off_qh * lse_stride_h\n    Delta += off_b * lse_stride_b + off_qh * lse_stride_h\n\n    # Modified load for k with 128bit (assume D1 % 4 == 0)\n    off_chunk_d1 = tl.arange(0, D1 // 4)\n    base_k = K + off_m[None, :] * k_stride_m + off_chunk_d1[:, None] * 4\n    pointers_k = base_k[:, :, None] + tl.arange(0, 4)[None, None, :]\n    mask_k = off_m[None, :, None] < M\n    k_vec = tl.load(pointers_k, mask=mask_k, other=0.)\n    k = tl.reshape(k_vec, [D1, BLOCK_SIZE_M], can_reorder=True)\n\n    # Modified load for v with 128bit (assume VD % 4 == 0)\n    off_chunk_vd = tl.arange(0, VD // 4)\n    base_v = V + off_m[None, :] * v_stride_m + off_chunk_vd[:, None] * 4\n    pointers_v = base_v[:, :, None] + tl.arange(0, 4)[None, None, :]\n    mask_v = off_m[None, :, None] < M\n    v_vec = tl.load(pointers_v, mask=mask_v, other=0.)\n    v = tl.reshape(v_vec, [VD, BLOCK_SIZE_M], can_reorder=True)\n\n    acc_dk = tl.zeros((BLOCK_SIZE_M, D1), dtype=tl.float32)\n    acc_dv = tl.zeros((BLOCK_SIZE_M, VD), dtype=tl.float32)\n\n    if D2 > 0:\n        # Modified load for k2 with 128bit (assume D2 % 4 == 0)\n        off_chunk_d2 = tl.arange(0, D2 // 4)\n        base_k2 = K + off_m[None, :] * k_stride_m + (off_chunk_d2[:, None] * 4 + D1)\n        pointers_k2 = base_k2[:, :, None] + tl.arange(0, 4)[None, None, :]\n        mask_k2 = off_m[None, :, None] < M\n        k2_vec = tl.load(pointers_k2, mask=mask_k2, other=0.)\n        k2 = tl.reshape(k2_vec, [D2, BLOCK_SIZE_M], can_reorder=True)\n        acc_dk2 = tl.zeros((BLOCK_SIZE_M, D2), dtype=tl.float32)\n\n    k_idx = off_m * stride + kernel_size - 1\n    for start_q_idx in range(start_m * stride + kernel_size - 1, N, BLOCK_SIZE_N):\n        off_n = start_q_idx + tl.arange(0, BLOCK_SIZE_N)\n\n        # Modified load for q with 128bit\n        base_q = Q + off_n[None, :] * q_stride_n + off_chunk_d1[:, None] * 4\n        pointers_q = base_q[:, :, None] + tl.arange(0, 4)[None, None, :]\n        mask_q = off_n[None, :, None] < N\n        q_vec = tl.load(pointers_q, mask=mask_q, other=0.)\n        q_resh = tl.reshape(q_vec, [D1, BLOCK_SIZE_N], can_reorder=True)\n        q = tl.trans(q_resh)\n\n        # Modified load for do with 128bit\n        base_do = DO + off_n[None, :] * do_stride_n + off_chunk_vd[:, None] * 4\n        pointers_do = base_do[:, :, None] + tl.arange(0, 4)[None, None, :]\n        mask_do = off_n[None, :, None] < N\n        do_vec = tl.load(pointers_do, mask=mask_do, other=0.)\n        do_resh = tl.reshape(do_vec, [VD, BLOCK_SIZE_N], can_reorder=True)\n        do = tl.trans(do_resh)\n\n        # Modified load for lse with 128bit (assume BLOCK_SIZE_N % 4 == 0)\n        off_chunk_n = tl.arange(0, BLOCK_SIZE_N // 4)\n        base_lse = Lse + (start_q_idx + off_chunk_n[:, None] * 4)\n        pointers_lse = base_lse[:, :, None] + tl.arange(0, 4)[None, None, :]\n        off_n_full = start_q_idx + off_chunk_n[:, None, None] * 4 + tl.arange(0, 4)[None, None, :]\n        mask_lse = off_n_full < N\n        lse_vec = tl.load(pointers_lse, mask=mask_lse, other=0.)\n        lse = tl.reshape(lse_vec, [BLOCK_SIZE_N], can_reorder=True)\n\n        # Modified load for delta with 128bit\n        base_delta = Delta + (start_q_idx + off_chunk_n[:, None] * 4)\n        pointers_delta = base_delta[:, :, None] + tl.arange(0, 4)[None, None, :]\n        mask_delta = off_n_full < N\n        delta_vec = tl.load(pointers_delta, mask=mask_delta, other=0.)\n        delta = tl.reshape(delta_vec, [BLOCK_SIZE_N], can_reorder=True)\n\n        attn_score = tl.dot(q, k) \n\n        if D2 > 0:\n            # Modified load for q2 with 128bit\n            base_q2 = Q + off_n[None, :] * q_stride_n + (off_chunk_d2[:, None] * 4 + D1)\n            pointers_q2 = base_q2[:, :, None] + tl.arange(0, 4)[None, None, :]\n            mask_q2 = off_n[None, :, None] < N\n            q2_vec = tl.load(pointers_q2, mask=mask_q2, other=0.)\n            q2_resh = tl.reshape(q2_vec, [D2, BLOCK_SIZE_N], can_reorder=True)\n            q2 = tl.trans(q2_resh)\n            attn_score = tl.dot(q2, k2, attn_score)\n\n        attn_score = tl.where(off_n[:, None] >= k_idx[None, :], attn_score, float('-inf'))\n        p = tl.exp(attn_score * sm_scale - lse[:, None])\n        \n        acc_dv = tl.dot(tl.trans(p, 1, 0).to(do.dtype), do, acc_dv)\n\n        dp = tl.dot(do, v)\n        ds = p * (dp - delta[:, None]) * sm_scale\n\n        acc_dk = tl.dot(tl.trans(ds, 1, 0).to(q.dtype), q, acc_dk)\n        if D2 > 0:\n            acc_dk2 = tl.dot(tl.trans(ds, 1, 0).to(q.dtype), q2, acc_dk2)\n\n    tl.store(DK + off_m[:, None] * dk_stride_m + tl.arange(0, D1)[None, :], acc_dk, mask=off_m[:, None] < M)\n    tl.store(DV + off_m[:, None] * dv_stride_m + tl.arange(0, VD)[None, :], acc_dv, mask=off_m[:, None] < M)\n    if D2 > 0:\n        tl.store(DK + off_m[:, None] * dk_stride_m + tl.arange(0, D2)[None, :] + D1, acc_dk2, mask=off_m[:, None] < M)\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N': bsn, 'BLOCK_SIZE_M': bsm}, num_stages=ns, num_warps=nw)\n#                  for bsm in [32, 64, 128]\n#                  for bsn in [32, 64, 128]\n#                  for ns in [1, 2]\n#                  for nw in [4, 8]\n#                  ], key=['N', \"M\"])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_M': 16}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_M': 16}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_M': 32}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_M': 32}, num_stages=2, num_warps=8),\n    ],\n    key=['N', 'M', 'D1', 'D2', 'VD']\n)\n@triton.jit\ndef _comp_attn_dq_kernel(DQ, DO, \n                Q, K, V, \n                Lse, Delta,\n                q_stride_b, q_stride_n, q_stride_h, q_stride_d,\n                k_stride_b, k_stride_m, k_stride_h, k_stride_d,\n                v_stride_b, v_stride_m, v_stride_h, v_stride_d,\n                do_stride_b, do_stride_n, do_stride_h, do_stride_d,\n                lse_stride_b, lse_stride_h, lse_stride_n,\n                sm_scale,  kernel_size, stride,\n                B, N, M, QH, KH, \n                D1: tl.constexpr, D2: tl.constexpr, VD: tl.constexpr, \n                BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_M: tl.constexpr\n                ):\n    start_n = tl.cast(tl.program_id(0), tl.int64) * BLOCK_SIZE_N\n    off_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    off_b = tl.cast(tl.program_id(1), tl.int64)\n    off_qh = tl.cast(tl.program_id(2), tl.int64)\n    \n    off_kh = off_qh // (QH // KH)\n\n    Q += off_b * q_stride_b + off_qh * q_stride_h\n    K += off_b * k_stride_b + off_kh * k_stride_h \n    V += off_b * v_stride_b + off_kh * v_stride_h\n    DQ += off_b * q_stride_b + off_qh * q_stride_h\n    DO += off_b * do_stride_b + off_qh * do_stride_h\n    Lse += off_b * lse_stride_b + off_qh * lse_stride_h\n    Delta += off_b * lse_stride_b + off_qh * lse_stride_h\n\n    q = tl.load(Q + off_n[:, None] * q_stride_n + tl.arange(0, D1), mask=off_n[:, None] < N, other=0.)\n    acc_dq = tl.zeros((BLOCK_SIZE_N, D1), dtype=tl.float32)\n    do = tl.load(DO + off_n[:, None] * do_stride_n + tl.arange(0, VD), mask=off_n[:, None] < N, other=0.)\n    lse = tl.load(Lse + off_n, mask=off_n < N, other=0.)\n    delta = tl.load(Delta + off_n, mask=off_n < N, other=0.)\n    if D2 > 0:\n        q2 = tl.load(Q + off_n[:, None] * q_stride_n + tl.arange(0, D2) + D1, mask=off_n[:, None] < N, other=0.)\n        acc_dq2 = tl.zeros((BLOCK_SIZE_N, D2), dtype=tl.float32)\n\n    off_m = tl.arange(0, BLOCK_SIZE_M)\n    for start_kv_idx in range(kernel_size-1, start_n + BLOCK_SIZE_N, BLOCK_SIZE_M * stride):\n\n        k = tl.load(K + off_m[None, :] * k_stride_m + tl.arange(0, D1)[:, None], mask=off_m[None, :] < M, other=0.)\n        v = tl.load(V + off_m[None, :] * v_stride_m + tl.arange(0, VD)[:, None], mask=off_m[None, :] < M, other=0.)\n        attn_score = tl.dot(q, k) \n        if D2 > 0:\n            k2 = tl.load(K + off_m[None, :] * k_stride_m + tl.arange(0, D2)[:, None] + D1, mask=off_m[None, :] < M, other=0.)\n            attn_score = tl.dot(q2, k2, attn_score)\n\n        k_idx = off_m * stride + kernel_size - 1\n        attn_score = tl.where(off_n[:, None] >= k_idx[None, :], attn_score, float('-inf'))\n        p = tl.exp(attn_score * sm_scale - lse[:, None])\n\n        dp = tl.dot(do, v)\n        ds = p * (dp - delta[:, None]) * sm_scale\n        \n        acc_dq = tl.dot(ds.to(k.dtype), tl.trans(k, 1, 0), acc_dq)\n        if D2 > 0:\n            acc_dq2 = tl.dot(ds.to(k.dtype), tl.trans(k2, 1, 0), acc_dq2)\n        off_m += BLOCK_SIZE_M\n\n    tl.store(DQ + off_n[:, None] * q_stride_n + tl.arange(0, D1)[None, :], acc_dq, mask=off_n[:, None] < N)\n    if D2 > 0:\n        tl.store(DQ + off_n[:, None] * q_stride_n + tl.arange(0, D2)[None, :] + D1, acc_dq2, mask=off_n[:, None] < N)\n\n\nclass _compress_attention(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, kernel_size, stride, sm_scale):\n        B, N, QH, D = q.shape\n        B2, M, KH, D2 = k.shape\n        B3, M2, KH2, VD = v.shape\n        assert B == B2 and B == B3 and M == M2 and D == D2 and KH == KH2\n        assert QH % KH == 0\n        assert math.log2(VD).is_integer()\n\n        if math.log2(D).is_integer():\n            D1 = D\n            D2 = 0\n        else:\n            D1 = 2**int(math.log2(D-1))\n            D2 = D - D1\n            assert math.log2(D2).is_integer()\n        if sm_scale is None:\n            sm_scale = D**-0.5\n        o = torch.empty(B, N, QH, VD, device=q.device, dtype=q.dtype)\n        lse = torch.empty(B, QH, N, dtype=torch.float32, device=q.device,)\n        grid = lambda meta: (B, QH, triton.cdiv(N, meta['BLOCK_SIZE_N']))\n        # Remove hardcoded kwargs - let autotune handle it\n        _comp_attn_fwd_kernel[grid](q, k, v, o, lse,\n                          *q.stride(),\n                          *k.stride(),\n                          *v.stride(),\n                          *o.stride(),\n                          *lse.stride(),\n                          sm_scale, kernel_size, stride,\n                          B, N, M, QH, KH, \n                          D1, D2, VD\n                          )\n        ctx.save_for_backward(q, k, v, o, lse)\n        ctx.infos = (B, N, M, QH, KH, D1, D2, VD, sm_scale, kernel_size, stride)\n        return o, lse\n\n    @staticmethod\n    def backward(ctx, do, *args):\n        assert do.is_contiguous()\n        B, N, M, QH, KH, D1, D2, VD, sm_scale, kernel_size, stride = ctx.infos\n        q, k, v, o, lse = ctx.saved_tensors\n        dq = torch.zeros_like(q)\n        dk = torch.empty(B, M, QH, D1+D2, device=q.device, dtype=q.dtype)\n        dv = torch.empty(B, M, QH, VD, device=q.device, dtype=q.dtype)\n\n        delta = torch.empty_like(lse)\n        grid = lambda meta: (B, QH, triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]))\n        # Remove hardcoded kwargs\n        _comp_attn_bwd_prepro[grid](o, do, delta,\n                              *o.stride(), \n                              *delta.stride(),\n                              N, VD\n                              )\n\n        grid = lambda meta: (triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]), B, QH)\n        # Remove hardcoded kwargs\n        _comp_attn_dkv_kernel[grid](dk, dv, do, \n                          q, k, v,\n                          lse, delta,\n                          *q.stride(),\n                          *k.stride(),\n                          *v.stride(),\n                          *dk.stride(),\n                          *dv.stride(),\n                          *do.stride(),\n                          *lse.stride(),\n                          sm_scale, kernel_size, stride,\n                          B, N, M, QH, KH, \n                          D1, D2, VD\n                          )\n        \n        grid = lambda meta: (triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]), B, QH)\n        # Remove hardcoded kwargs\n        _comp_attn_dq_kernel[grid](dq, do, \n                          q, k, v,\n                          lse, delta,\n                          *q.stride(),\n                          *k.stride(),\n                          *v.stride(),\n                          *do.stride(),\n                          *lse.stride(),\n                          sm_scale, kernel_size, stride,\n                          B, N, M, QH, KH, \n                          D1, D2, VD\n                          )   \n        dk = dk.view(B, M, KH, -1, D1+D2).sum(3)\n        dv = dv.view(B, M, KH, -1, VD).sum(3)\n        return dq, dk, dv, None, None, None\n\n\n\ndef compress_attn(q, k, v, kernel_size, stride, sm_scale=None):\n    return _compress_attention.apply(q, k, v, kernel_size, stride, sm_scale)\n\n\nclass CompressKV(torch.nn.Module):\n    def __init__(self, head_dim, kernel_size, stride):\n        super().__init__()\n        self.head_dim = head_dim\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.pe = torch.nn.Parameter(torch.randn(kernel_size, head_dim))\n        self.weight = torch.nn.Parameter(torch.randn(kernel_size,))\n\n    def forward(self, x):\n        return blcok_compress(x, self.weight, self.pe, self.stride)\n    \nclass CompressAttn(torch.nn.Module):\n    def __init__(self, qk_head_dim, v_head_dim, kernel_size, stride):\n        super().__init__()\n        self.qk_head_dim = qk_head_dim\n        self.v_head_dim = v_head_dim\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.compress_key = CompressKV(self.qk_head_dim, kernel_size, stride)\n        self.compress_value = CompressKV(self.v_head_dim, kernel_size, stride)\n        self.sm_scale = qk_head_dim ** -0.5\n\n    def forward(self, q, k, v):\n        cmp_k = self.compress_key(k)\n        cmp_v = self.compress_value(v)\n        o, lse = compress_attn(q, cmp_k, cmp_v, self.kernel_size, self.stride, self.sm_scale)\n        return o, lse, cmp_k\n\n# [2022-10-23] Downloaded from https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py\n# for benchmarking.\n# We fixed a few dtype cast to make it work for bf16\n\n\"\"\"\nFused Attention\n===============\nThis is a Triton implementation of the Flash Attention algorithm\n(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n\"\"\"\n\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_M': bsm, 'BLOCK_SIZE_N': bsn}, num_stages=ns, num_warps=nw)\n#                  for bsm in [32, 64, 128]\n#                  for bsn in [32, 64, 128]\n#                  for ns in [1, 2, 4]\n#                  for nw in [4, 8]\n#                  ], key=['N', \"M\"])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=2, num_warps=8),\n    ],\n    key=['N', 'M']\n)\n@triton.jit\ndef _compute_attn_probs(Q, K, Lse, P,\n                q_stride_b, q_stride_n, q_stride_h, q_stride_d,\n                k_stride_b, k_stride_m, k_stride_h, k_stride_d,\n                lse_stride_b, lse_stride_h, lse_stride_n,\n                p_stride_b, p_stride_h, p_stride_n, p_stride_m,\n                sm_scale, kernel_size, stride,\n                B, N, M, KH, nrep,\n                D1: tl.constexpr, D2: tl.constexpr, \n                BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):\n    start_n = tl.cast(tl.program_id(1), tl.int64) * BLOCK_SIZE_N\n    start_m = tl.cast(tl.program_id(2), tl.int64) * BLOCK_SIZE_M\n    if (start_n + BLOCK_SIZE_N) < (start_m * stride + kernel_size):\n        return  \n    off_bh = tl.cast(tl.program_id(0), tl.int64)\n    off_kh = off_bh % KH\n    off_b = off_bh // KH\n    off_qh = off_kh * nrep\n\n    off_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    off_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n\n    Q += off_b * q_stride_b + off_qh * q_stride_h\n    K += off_b * k_stride_b + off_kh * k_stride_h\n    Lse += off_b * lse_stride_b + off_qh * lse_stride_h\n    P += off_b * p_stride_b + off_kh * p_stride_h\n\n\n    k = tl.load(K + off_m[None, :] * k_stride_m + tl.arange(0, D1)[:, None], mask=off_m[None, :]<M)\n    if D2 > 0:\n        k2 = tl.load(K + off_m[None, :] * k_stride_m + tl.arange(0, D2)[:, None] + D1, mask=off_m[None, :]<M)\n\n    k_idx = off_m * stride + kernel_size - 1\n    causal_mask = off_n[:, None] >= k_idx[None, :]\n    p = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_M), dtype=tl.float32)\n    for h_idx in range(nrep):\n        q = tl.load(Q + h_idx * q_stride_h + off_n[:, None] * q_stride_n + tl.arange(0, D1)[None, :], mask=off_n[:, None] < N, other=0.)\n        lse = tl.load(Lse + h_idx * lse_stride_h + off_n * lse_stride_n, mask=off_n < N, other=0.)\n        attn_score = tl.dot(q, k)\n        if D2 > 0:\n            q2 = tl.load(Q + h_idx * q_stride_h + off_n[:, None] * q_stride_n + tl.arange(0, D2)[None, :] + D1, mask=off_n[:, None] < N, other=0.)\n            attn_score = tl.dot(q2, k2, attn_score)\n        attn_score = tl.where(causal_mask, attn_score, float('-inf'))\n        p += tl.exp(attn_score * sm_scale - lse[:, None])\n        tl.store(P + off_n[:, None] * p_stride_n + off_m[None, :] * p_stride_m, p, mask=(off_n[:, None] < N) & (off_m[None, :] < M))\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N': bs,}, num_stages=ns, num_warps=nw)\n#                  for bs in [1, 2, 4, 8, 16, 32]\n#                  for ns in [1, 2, 4]\n#                  for nw in [4, 8]\n#                  ], key=['N', \"M\"])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 1}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 2}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 4}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 16}, num_stages=2, num_warps=4),\n    ],\n    key=['N', 'M']\n)\n@triton.jit\ndef _compute_select_probs(AP, SP, FInd, BInd,\n                          ap_stride_b, ap_stride_h, ap_stride_n, ap_stride_m,\n                          sp_stride_b, sp_stride_h, sp_stride_n, sp_stride_k,\n                          find_stride_b, find_stride_h, find_stride_n, find_stride_k,\n                          bind_stride_b, bind_stride_h, bind_stride_k, bind_stride_n,\n                          kernel_size, stride, \n                          select_size, num_selcct_blocks, top_n, return_p: tl.constexpr,\n                          B, N, M, KH, \n                          BLOCK_SIZE_K: tl.constexpr,\n                          BLOCK_SIZE_N: tl.constexpr,\n                          CHUNK_N: tl.constexpr=128\n                            ):\n    off_bh = tl.cast(tl.program_id(0), tl.int64)\n    off_h = off_bh % KH\n    off_b = off_bh // KH\n    start_n = tl.cast(tl.program_id(1), tl.int64) * CHUNK_N \\\n            + tl.program_id(2) * BLOCK_SIZE_N\n    if start_n >= N:\n        return\n    off_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n\n\n    AP += off_b * ap_stride_b + off_h * ap_stride_h\n    SP += off_b * sp_stride_b + off_h * sp_stride_h\n    FInd += off_b * find_stride_b + off_h * find_stride_h\n    BInd += off_b * bind_stride_b + off_h * bind_stride_h\n\n    acc_p = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_K), dtype=tl.float32)\n    select_idx = tl.arange(0, BLOCK_SIZE_K)\n\n    select_start = 0\n    select_end = select_size\n    compress_start = stride - kernel_size \n    # num_loops = (select_size + 2 * (kernel_size - stride) - kernel_size) // stride + 1\n    num_loops = (select_size + kernel_size - stride) // stride\n    # tl.static_print(num_loops)\n    compress_idx = (select_idx * select_size - kernel_size) // stride + 1\n    for _ in range(num_loops):\n        compress_end = compress_start + kernel_size\n        area = tl.minimum(compress_end, select_end) - tl.maximum(compress_start, select_start)\n        w = area / stride\n        mask = (compress_idx >= 0) & (compress_idx < M)\n        p = tl.load(AP + off_n[:, None] * ap_stride_n + compress_idx[None, :] * ap_stride_m, \n                    mask=(off_n[:, None] < N) & mask[None, :], other=0.) * w\n        acc_p += p\n        compress_idx += 1\n        compress_start += stride\n        \n    if return_p:\n        acc_p = tl.where(tl.arange(0, BLOCK_SIZE_K)[None, :] == (off_n // select_size)[:, None], 9999, acc_p)\n        tl.store(SP + off_n[:, None] * sp_stride_n + select_idx[None, :] * sp_stride_k, \n                  acc_p, mask=(off_n[:, None] < N) & (select_idx[None, :] < num_selcct_blocks))\n    tl.store(BInd + off_n * bind_stride_n + (off_n // select_size) * bind_stride_k, off_n + 1, mask=off_n < N)\n    tl.store(FInd + off_n * find_stride_n, off_n // select_size, mask=off_n < N)\n    acc_p = tl.where(tl.arange(0, BLOCK_SIZE_K)[None, :] == (off_n // select_size)[:, None],\n                     -1., acc_p)\n    top_n = tl.minimum(top_n, (start_n + BLOCK_SIZE_N - 1) // select_size + 1)\n\n    for i in range(1, top_n):\n        max_idx = tl.argmax(acc_p, axis=-1)\n        tl.store(BInd + off_n * bind_stride_n + max_idx * bind_stride_k, off_n + 1, mask=off_n < N)\n        tl.store(FInd + off_n * find_stride_n + i * find_stride_k, max_idx, mask=off_n < N)\n        acc_p = tl.where(tl.arange(0, BLOCK_SIZE_K)[None, :] == max_idx[:, None],\n                    -1., acc_p)\n\n\n@torch.inference_mode()\ndef select_for_fwd_bwd(q, k, lse, kernel_size, stride, select_size, top_n, sm_scale=None, return_p=False):\n    B, N, QH, D = q.shape\n    B2, M, KH, D2 = k.shape\n    assert QH % KH == 0\n    nrep = QH // KH\n\n    if math.log2(D).is_integer():\n        D1 = D\n        D2 = 0\n    else:\n        D1 = 2**int(math.log2(D-1))\n        D2 = D - D1\n        assert math.log2(D2).is_integer()\n\n    if sm_scale is None:\n        sm_scale = D**-0.5\n\n    num_selcct_blocks = triton.cdiv(N, select_size)\n    top_n = min(num_selcct_blocks, top_n)\n\n    probs = torch.zeros(B, KH, N, M, device=q.device, dtype=torch.float16)\n    \n    # Remove hardcoded kwargs\n    grid = lambda meta: (B*KH, triton.cdiv(N, meta['BLOCK_SIZE_N']), triton.cdiv(M, meta['BLOCK_SIZE_M']))\n    _compute_attn_probs[grid](q, k, lse, probs,\n                        *q.stride(),\n                        *k.stride(),\n                        *lse.stride(),\n                        *probs.stride(),\n                        sm_scale, kernel_size, stride,\n                        B, N, M, KH, nrep,\n                        D1, D2\n                        )\n    \n    BLOCK_SIZE_K = triton.next_power_of_2(num_selcct_blocks)\n    select_probs = None\n    if return_p:\n        select_probs = torch.zeros(B, KH, N, num_selcct_blocks, device=probs.device, dtype=torch.float16)\n    fwd_ind = torch.full((B, KH, N, top_n), num_selcct_blocks, dtype=torch.int32, device=probs.device)\n    bwd_ind = torch.zeros(B, KH, num_selcct_blocks, N, dtype=torch.int32, device=probs.device)\n    \n    grid=lambda meta: (B * KH, triton.cdiv(N, meta['CHUNK_N']), triton.cdiv(meta['CHUNK_N'], meta['BLOCK_SIZE_N']))\n    _compute_select_probs[grid](probs, select_probs if return_p else probs, fwd_ind, bwd_ind,\n                                *probs.stride(),\n                                *(select_probs.stride() if return_p else probs.stride()),\n                                *fwd_ind.stride(),\n                                *bwd_ind.stride(),\n                                kernel_size, stride, \n                                select_size, num_selcct_blocks, top_n, return_p,\n                                B, N, M, KH,\n                                BLOCK_SIZE_K\n                                )\n    return select_probs, fwd_ind, bwd_ind\n\n\n\n\n\n\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N': bsn}, num_stages=ns, num_warps=nw)\n#                  for bsn in [128, 256, 512, 1024]\n#                  for ns in [1, 2, 4]\n#                  for nw in [4, 8]\n#                  ], key=['N']) \n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 64}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 128}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 256}, num_stages=2, num_warps=4),\n    ],\n    key=['N']\n)\n@triton.jit\ndef _fix_bwd_indices(Ind, Cnt,\n                ind_stride_b, ind_stride_h, ind_stride_k, ind_stride_n,\n                cnt_stride_b, cnt_stride_h, cnt_stride_k,\n                N,\n                BLOCK_SIZE_N: tl.constexpr, \n                ):\n    \n    off_b = tl.cast(tl.program_id(0), tl.int64)\n    off_h = tl.cast(tl.program_id(1), tl.int64)\n    off_k = tl.cast(tl.program_id(2), tl.int64)\n\n    Ind += off_b * ind_stride_b + off_h * ind_stride_h + off_k * ind_stride_k\n    Cnt += off_b * cnt_stride_b + off_h * cnt_stride_h + off_k * cnt_stride_k\n\n    last_cnt = 0\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    for start_n in range(0, N, BLOCK_SIZE_N):\n        off_n = start_n + cols\n        ind = tl.load(Ind + off_n, mask=off_n < N, other=0)\n        this_cnt = tl.sum(ind)\n        if this_cnt > 0:\n            this_cnt = tl.sum(tl.where(ind == 0, 0, 1))\n            ind = tl.sort(ind, descending=True)\n            tl.store(Ind + last_cnt + cols, ind - 1, mask=cols < this_cnt)\n            last_cnt += this_cnt\n    tl.store(Cnt, last_cnt)\n\nfrom copy import deepcopy\n\n\n@torch.inference_mode()\ndef fix_bwd_ind(bwd_ind, inplace=True):\n    assert bwd_ind.is_contiguous()\n    if not inplace:\n        bwd_ind = deepcopy(bwd_ind)\n    B, KH, num_selcct_blocks, N = bwd_ind.shape\n    count = torch.empty(B, KH, num_selcct_blocks, dtype=torch.int32, device=bwd_ind.device)\n    # Remove hardcoded kwargs\n    _fix_bwd_indices[(B, KH, num_selcct_blocks)](bwd_ind, count,\n                                                 *bwd_ind.stride(),\n                                                 *count.stride(),\n                                                 N\n                                                 )\n    return bwd_ind, count\n\n\n# @triton.autotune([triton.Config({}, num_warps=nw)\n#                 #  for ns in [1, 2, 4]\n#                  for nw in [1, 2, 4, 8, 16]\n#                  ], key=['D1', \"D2\", 'VD', 'BLOCK_SIZE_H', 'BLOCK_SIZE_M'])\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n    ],\n    key=['D1', 'D2', 'VD', 'BLOCK_SIZE_H', 'BLOCK_SIZE_M']\n)\n@triton.jit\ndef _select_attn_fwd_kernel(Q, \n                K, \n                V, \n                O, \n                Lse, \n                Ind,\n                sm_scale, \n                top_n: tl.constexpr,\n                N, \n                M, \n                KH: tl.constexpr,\n                QH: tl.constexpr, \n                D1: tl.constexpr, \n                D2: tl.constexpr, \n                VD: tl.constexpr, \n                BLOCK_SIZE_H: tl.constexpr=16, \n                BLOCK_SIZE_M: tl.constexpr=64,\n                CHUNK_N: tl.constexpr=64):\n    off_n = tl.program_id(0) * CHUNK_N + tl.program_id(1)\n    if off_n >= N:\n        return\n    off_bh = tl.program_id(2)\n    off_kh = off_bh % KH\n    off_b = off_bh // KH\n\n    D = D1 + D2\n    nrep = QH // KH\n    strat_qh = nrep * off_kh\n    \n    Q += (off_b * N + off_n) * QH * D + strat_qh * D\n    O += (off_b * N + off_n) * QH * VD + strat_qh * VD\n    K += (off_b * M * KH + off_kh) * D\n    V += (off_b * M * KH + off_kh) * VD\n    Ind += (off_b * N * KH + off_n + off_kh * N) * top_n\n    Lse += (off_b * N + off_n) * QH\n\n\n    q_ptrs = tl.make_block_ptr(Q, (nrep, D), (D, 1), (0, 0),(BLOCK_SIZE_H, D1), (1,0))\n    q = tl.load(q_ptrs, boundary_check=(0,1))\n    if D2 > 0:\n        q_ptrs2 = tl.make_block_ptr(Q, (nrep, D), (D, 1), (0, D1),(BLOCK_SIZE_H, D2), (1,0))\n        q2 = tl.load(q_ptrs2, boundary_check=(0,1))\n\n    m_i = tl.full([BLOCK_SIZE_H], float(\"-inf\"), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_SIZE_H], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_SIZE_H, VD], dtype=tl.float32)\n\n    stop_n = tl.minimum(top_n, tl.cdiv(off_n+1, BLOCK_SIZE_M))\n    for i in range(0, stop_n):\n        start_m = tl.load(Ind + i).to(tl.int32) * BLOCK_SIZE_M\n        k_ptrs = tl.make_block_ptr(K, (D, M), (1, KH * D), (0, start_m), (D1, BLOCK_SIZE_M), (0,1))\n        v_ptrs = tl.make_block_ptr(V, (M, VD), (KH * VD , 1), (start_m, 0), (BLOCK_SIZE_M, VD), (1, 0))\n        k = tl.load(k_ptrs, boundary_check=(0, 1))\n        v = tl.load(v_ptrs, boundary_check=(0, 1))\n        attn_score = tl.dot(q, k)\n        if D2>0:\n            k_ptrs2 = tl.make_block_ptr(K, (D, M), (1, KH * D), (D1, start_m), (D2, BLOCK_SIZE_M), (0,1))\n            k2 = tl.load(k_ptrs2, boundary_check=(0, 1))\n            attn_score = tl.dot(q2, k2, attn_score)\n        attn_score *= sm_scale\n\n        attn_score = tl.where(off_n >= (start_m + tl.arange(0, BLOCK_SIZE_M))[None, :], attn_score, float('-inf'))\n\n        new_m_i = tl.maximum(m_i, tl.max(attn_score, axis=1))\n        alpha = tl.exp(m_i - new_m_i)\n\n        exp_attn_score = tl.exp(attn_score - new_m_i[:, None])\n\n        l_i = l_i * alpha + tl.sum(exp_attn_score, axis=-1)\n\n        acc = acc * alpha[:, None] + tl.dot(exp_attn_score.to(v.dtype), v)\n        m_i = new_m_i\n\n\n    acc /= l_i[:, None]\n    o_ptrs = tl.make_block_ptr(O, (nrep, VD), (VD, 1), (0, 0),(BLOCK_SIZE_H, VD), (1,0))\n    tl.store(o_ptrs, acc.to(o_ptrs.dtype.element_ty), boundary_check=(0,1))\n    lse = m_i + tl.log(l_i)\n    tl.store(Lse + strat_qh + tl.arange(0, BLOCK_SIZE_H), lse, mask=tl.arange(0, BLOCK_SIZE_H) < nrep)\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N': bsn}, num_stages=ns, num_warps=nw)\n#                  for bsn in [16, 32, 64, 128]\n#                  for ns in [1, 2, 4]\n#                  for nw in [4, 8]\n#                  ], key=['N', \"M\"])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 16}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64}, num_stages=1, num_warps=8),\n    ],\n    key=['N', 'VD']  # Also fix the key - VD is used in the kernel\n)\n@triton.jit\ndef _select_attn_bwd_prepo_kernel(O,DO,Delta,\n                    o_stride_b, o_stride_n, o_stride_h, o_stride_d,\n                    delta_stride_b, delta_stride_n, delta_stride_h,\n                    N, VD: tl.constexpr,\n                    BLOCK_SIZE_N: tl.constexpr\n                    ):\n    off_b = tl.cast(tl.program_id(0), tl.int64)\n    off_h = tl.cast(tl.program_id(1), tl.int64)\n    off_n = tl.cast(tl.program_id(2), tl.int64) * BLOCK_SIZE_N\n\n    O += off_b * o_stride_b + off_h * o_stride_h\n    DO += off_b * o_stride_b + off_h * o_stride_h\n    Delta += off_b * delta_stride_b + off_h * delta_stride_h\n    \n    rows = tl.arange(0, BLOCK_SIZE_N) + off_n\n    row_mask = rows < N\n    cols = tl.arange(0, VD)\n    \n    o = tl.load(O + rows[:, None] * o_stride_n + cols[None, :], mask=row_mask[:, None], other=0.).to(tl.float32)\n    do = tl.load(DO + rows[:, None] * o_stride_n + cols[None, :], mask=row_mask[:, None], other=0.).to(tl.float32)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + rows * delta_stride_n, delta, mask=row_mask)\n\n\n# @triton.autotune([triton.Config({'BLOCK_SIZE_N':bsn}, num_stages=ns, num_warps=nw)\n#                  for bsn in [32, 64, 128]\n#                  for ns in [1, 2, 4]\n#                  for nw in [4, 8]\n#                  ], key=['D1', 'D2'])\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32}, num_stages=1, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64}, num_stages=1, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128}, num_stages=1, num_warps=4),\n    ],\n    key=['D1', 'D2']\n)\n@triton.jit\ndef _select_attn_bwd_dkv_kernel(DK, DV, DO, \n                Q, K, V, \n                Lse, Delta, Ind, Count,\n                q_stride_b, q_stride_n, q_stride_h, q_stride_d,\n                k_stride_b, k_stride_m, k_stride_h, k_stride_d,\n                v_stride_b, v_stride_m, v_stride_h, v_stride_d,\n                dk_stride_b, dk_stride_m, dk_stride_h, dk_stride_d,\n                dv_stride_b, dv_stride_m, dv_stride_h, dv_stride_d,\n                do_stride_b, do_stride_n, do_stride_h, do_stride_d,\n                lse_stride_b, lse_stride_n, lse_stride_h,\n                ind_stride_b, ind_stride_h, ind_stride_m, ind_stride_n,\n                cnt_stride_b, cnt_stride_h, cnt_stride_m,\n                sm_scale, \n                N, M, nrep,  \n                D1: tl.constexpr, D2: tl.constexpr, VD: tl.constexpr, \n                BLOCK_SIZE_M: tl.constexpr,BLOCK_SIZE_N: tl.constexpr,\n                ):\n    pid0 = tl.cast(tl.program_id(0), tl.int64) \n    off_m = pid0 * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    off_b = tl.cast(tl.program_id(1), tl.int64)\n    off_qh = tl.cast(tl.program_id(2), tl.int64)\n    off_kh = off_qh // nrep\n\n    Q += off_b * q_stride_b + off_qh * q_stride_h\n    K += off_b * k_stride_b + off_kh * k_stride_h\n    V += off_b * v_stride_b + off_kh * v_stride_h\n    DK += off_b * dk_stride_b + off_qh * dk_stride_h\n    DV += off_b * dv_stride_b + off_qh * dv_stride_h\n    DO += off_b * do_stride_b + off_qh * do_stride_h\n    Lse += off_b * lse_stride_b + off_qh * lse_stride_h \n    Delta += off_b * lse_stride_b + off_qh * lse_stride_h\n    Ind += off_b * ind_stride_b + off_kh * ind_stride_h + pid0 * ind_stride_m\n    Count += off_b * cnt_stride_b + off_kh * cnt_stride_h\n\n    k = tl.load(K + off_m[None, :] * k_stride_m + tl.arange(0, D1)[:, None], mask=off_m[None, :] < M, other=0.)\n    v = tl.load(V + off_m[None, :] * v_stride_m + tl.arange(0, VD)[:, None], mask=off_m[None, :] < M, other=0.)\n    acc_dk = tl.zeros((BLOCK_SIZE_M, D1), dtype=tl.float32)\n    acc_dv = tl.zeros((BLOCK_SIZE_M, VD), dtype=tl.float32)\n\n    if D2 > 0:\n        k2 = tl.load(K + off_m[None, :] * k_stride_m + tl.arange(0, D2)[:, None] + D1, mask=off_m[None, :] < M, other=0.)\n        acc_dk2 = tl.zeros((BLOCK_SIZE_M, D2), dtype=tl.float32)\n\n    count = tl.load(Count + pid0)\n    for start in range(0, count, BLOCK_SIZE_N):\n        off_ind = start + tl.arange(0, BLOCK_SIZE_N)\n        q_idx = tl.load(Ind + off_ind, off_ind < count, other=0)\n        q_idx = tl.where(off_ind < count, q_idx, N)\n        q = tl.load(Q + q_idx[:, None] * q_stride_n + tl.arange(0, D1)[None, :], mask=q_idx[:, None] < N, other=0.)\n        lse = tl.load(Lse + q_idx * lse_stride_n, mask=q_idx < N, other=0.)\n        attn_score = tl.dot(q, k)\n        if D2 > 0:\n            q2 = tl.load(Q + q_idx[:, None] * q_stride_n + tl.arange(0, D2)[None, :] + D1, mask=q_idx[:, None] < N, other=0.)\n            attn_score = tl.dot(q2, k2, attn_score)\n\n        attn_score = tl.where(q_idx[:, None] >= off_m[None, :], attn_score, float('-inf'))\n        \n        p = tl.exp(attn_score * sm_scale - lse[:, None])\n        \n        do = tl.load(DO + q_idx[:, None] * do_stride_n + tl.arange(0, VD)[None, :], mask=q_idx[:, None] < N, other=0.)\n        \n        acc_dv = tl.dot(tl.permute(p, 1, 0).to(do.dtype), do, acc_dv)\n        \n        \n        delta = tl.load(Delta + q_idx * lse_stride_n, mask=q_idx < N, other=0.)\n        dp = tl.dot(do, v)\n        ds = p * (dp - delta[:, None]) * sm_scale\n        acc_dk = tl.dot(tl.permute(ds, 1, 0).to(q.dtype), q, acc_dk)\n        if D2 > 0:\n            acc_dk2 = tl.dot(tl.trans(ds, 1, 0).to(q.dtype), q2, acc_dk2)\n    \n    tl.store(DK + off_m[:, None] * dk_stride_m + tl.arange(0, D1)[None, :], acc_dk, mask=off_m[:, None] < M)\n    tl.store(DV + off_m[:, None] * dv_stride_m + tl.arange(0, VD)[None, :], acc_dv, mask=off_m[:, None] < M)\n    if D2 > 0:\n        tl.store(DK + off_m[:, None] * dk_stride_m + tl.arange(0, D2)[None, :] + D1, acc_dk2, mask=off_m[:, None] < M)\n\n# @triton.autotune([triton.Config({}, num_warps=nw)\n#                  for nw in [1, 2, 4, 8]\n#                  ], key=['D1', 'D2'])\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n    ],\n    key=['D1', 'D2']\n)\n@triton.jit\ndef _select_attn_bwd_dq_kernel( DQ, \n                DO, \n                Q, \n                K, \n                V, \n                Lse, \n                Delta, \n                Ind, \n                sm_scale, \n                top_n,\n                N: tl.constexpr, \n                M: tl.constexpr, \n                QH: tl.constexpr,\n                KH: tl.constexpr, \n                D1: tl.constexpr, \n                D2: tl.constexpr, \n                VD: tl.constexpr, \n                BLOCK_SIZE_H: tl.constexpr, \n                BLOCK_SIZE_M: tl.constexpr,\n                CHUNK_N: tl.constexpr=128,\n                ):\n    off_n = tl.program_id(0) * CHUNK_N + tl.program_id(1)\n    if off_n >= N:\n        return\n    off_bh = tl.program_id(2)\n    off_kh = off_bh % KH\n    off_b = off_bh // KH\n\n    D = D1 + D2\n    nrep = QH // KH\n    off_qh = off_kh * nrep\n\n    Q += (off_b * N * QH + off_n * QH + off_qh) * D\n    K += (off_b * M * KH + off_kh) * D\n    V += (off_b * M * KH + off_kh) * VD\n    DQ += (off_b * N * QH + off_n * QH + off_qh) * D\n    DO += (off_b * N * QH + off_n * QH + off_qh) * VD\n    Lse += (off_b * N + off_n) * QH + off_qh\n    Delta += (off_b * N + off_n) * QH + off_qh\n    Ind += (off_b * N * KH + off_n + off_kh * N) * top_n\n\n    \n    q_ptrs = tl.make_block_ptr(Q, (nrep, D), (D, 1), (0, 0),(BLOCK_SIZE_H, D1), (1,0))\n    do_ptrs = tl.make_block_ptr(DO, (nrep, VD), (VD, 1), (0, 0),(BLOCK_SIZE_H, VD), (1,0))\n    q = tl.load(q_ptrs, boundary_check=(0,1))\n    do = tl.load(do_ptrs, boundary_check=(0, 1))\n    heads = tl.arange(0, BLOCK_SIZE_H)\n    lse = tl.load(Lse + heads, mask=heads<nrep, other=0.)\n    delta = tl.load(Delta + heads, mask=heads<nrep, other=0.)\n    acc_dq = tl.zeros([BLOCK_SIZE_H, D1], dtype=tl.float32)\n\n    if D2 > 0:\n        q_ptrs2 = tl.make_block_ptr(Q, (nrep, D), (D, 1), (0, D1),(BLOCK_SIZE_H, D2), (1,0))\n        q2 = tl.load(q_ptrs2, boundary_check=(0,1))\n        acc_dq2 = tl.zeros([BLOCK_SIZE_H, D2], dtype=tl.float32)\n\n    \n    stop_n = tl.minimum(top_n, tl.cdiv(off_n+1, BLOCK_SIZE_M))\n    for i in range(0, stop_n):\n        select_idx = tl.load(Ind + i)\n        start_m = select_idx * BLOCK_SIZE_M\n        k_ptrs = tl.make_block_ptr(K, (D, M), (1, KH * D), (0, start_m), (D1, BLOCK_SIZE_M), (0, 1))\n        v_ptrs = tl.make_block_ptr(V, (VD, M), (1, KH * VD), (0, start_m), (VD, BLOCK_SIZE_M), (0, 1))\n        k = tl.load(k_ptrs, boundary_check=(0,1))\n        attn_score = tl.dot(q, k)\n        if D2>0:\n            k_ptrs2 = tl.make_block_ptr(K, (D, M), (1, KH * D), (D1, start_m), (D2, BLOCK_SIZE_M), (0, 1))\n            k2 = tl.load(k_ptrs2, boundary_check=(0,1))\n            attn_score = tl.dot(q2, k2, attn_score)\n        v = tl.load(v_ptrs, boundary_check=(0,1))\n        dp = tl.dot(do, v)\n        \n        attn_score = tl.where(off_n >= (start_m + tl.arange(0, BLOCK_SIZE_M))[None, :], attn_score, float('-inf'))\n        p = tl.exp(attn_score * sm_scale - lse[:, None])\n    \n        ds = p * (dp - delta[:, None]) * sm_scale\n\n        acc_dq = tl.dot(ds.to(k.dtype), tl.trans(k, 1, 0), acc_dq)\n        if D2 > 0:\n            acc_dq2 = tl.dot(ds.to(k.dtype), tl.trans(k2, 1, 0), acc_dq2)\n    \n    dq_ptrs = tl.make_block_ptr(DQ, (nrep, D), (D, 1), (0, 0),(BLOCK_SIZE_H, D1), (1,0))\n    tl.store(dq_ptrs, acc_dq.to(dq_ptrs.dtype.element_ty), boundary_check=(0, 1))\n    if D2 > 0:\n        dq_ptrs2 = tl.make_block_ptr(DQ, (nrep, D), (D, 1), (0, D1),(BLOCK_SIZE_H, D2), (1,0))\n        tl.store(dq_ptrs2, acc_dq2.to(dq_ptrs.dtype.element_ty), boundary_check=(0, 1))\n\n\n\nclass _select_attention(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, select_size, fwd_ind, bwd_ind, sm_scale, inplace):\n        B, N, QH, D = q.shape\n        B2, M, KH, D2 = k.shape\n        B3, M2, KH2, VD = v.shape\n        assert B == B2 and B == B3 and M == M2 and D == D2 and KH == KH2\n        assert QH % KH == 0\n        assert math.log2(VD).is_integer()\n        assert math.log2(select_size).is_integer()\n        if math.log2(D).is_integer():\n            D1 = D\n            D2 = 0\n        else:\n            D1 = 2**int(math.log2(D-1))\n            D2 = D - D1\n            assert math.log2(D2).is_integer()\n\n        if sm_scale is None:\n            sm_scale = D**-0.5\n\n        o = torch.empty(B, N, QH, VD, device=q.device, dtype=q.dtype)\n        lse = torch.empty(B, N,QH, dtype=torch.float32, device=q.device,)\n\n        nrep = QH // KH\n        BLOCK_SIZE_H = max(triton.next_power_of_2(nrep), 16)\n        BLOCK_SIZE_M = select_size\n        top_n = fwd_ind.size(-1)\n        # Remove hardcoded kwargs\n        grid = lambda meta: (triton.cdiv(N, meta['CHUNK_N']), meta['CHUNK_N'], B * KH)\n        _select_attn_fwd_kernel[grid](q, k, v, o, lse, fwd_ind,\n                        sm_scale, top_n,\n                        N, M, KH, QH,\n                        D1, D2, VD,\n                        BLOCK_SIZE_H, BLOCK_SIZE_M\n                        )\n        ctx.save_for_backward(q, k, v, o, lse)\n        ctx.bwd_ind = bwd_ind\n        ctx.fwd_ind = fwd_ind\n        ctx.infos = (B, N, M, QH, KH, D1, D2, VD, sm_scale, nrep, top_n, BLOCK_SIZE_H, BLOCK_SIZE_M)\n        ctx.inplace = inplace\n        return o\n\n    @staticmethod\n    def backward(ctx, do, *args):\n        assert do.is_contiguous()\n        bwd_ind, count = fix_bwd_ind(ctx.bwd_ind, ctx.inplace)\n        B, N, M, QH, KH, D1, D2, VD, sm_scale, nrep, top_n, BLOCK_SIZE_H, BLOCK_SIZE_M = ctx.infos\n        q, k, v, o, lse = ctx.saved_tensors\n        \n        delta = torch.empty_like(lse)\n        grid = lambda meta: (B, QH, triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]))\n        # Remove hardcoded kwargs\n        _select_attn_bwd_prepo_kernel[grid](o, do, delta,\n                              *o.stride(), \n                              *delta.stride(),\n                              N, VD\n                              )\n        \n        dk = torch.empty(B, M, QH, D1+D2, device=k.device, dtype=k.dtype)\n        dv = torch.empty(B, M, QH, VD, device=k.device, dtype=k.dtype)\n        # Remove hardcoded kwargs\n        grid = lambda meta: (triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]), B, QH)\n        _select_attn_bwd_dkv_kernel[grid](dk, dv, do, \n                          q, k, v,\n                          lse, delta, \n                          bwd_ind,\n                          count,\n                          *q.stride(),\n                          *k.stride(),\n                          *v.stride(),\n                          *dk.stride(),\n                          *dv.stride(),\n                          *do.stride(),\n                          *lse.stride(),\n                          *bwd_ind.stride(),\n                          *count.stride(),\n                          sm_scale, \n                          N, M, nrep,  \n                          D1, D2, VD,\n                          BLOCK_SIZE_M\n                          )\n        dk = dk.view(B, M, KH, nrep, -1).sum(3)\n        dv = dv.view(B, M, KH, nrep, -1).sum(3)\n        \n        dq = torch.empty_like(q)\n        # Remove hardcoded kwargs\n        grid = lambda meta: (triton.cdiv(N, meta['CHUNK_N']), meta['CHUNK_N'], B * KH)\n        _select_attn_bwd_dq_kernel[grid](dq, do, \n                          q, k, v,\n                          lse, delta, \n                          ctx.fwd_ind,\n                          sm_scale, top_n,\n                          N, M, QH,KH,  \n                          D1, D2, VD,\n                          BLOCK_SIZE_H, BLOCK_SIZE_M\n                          )\n        return dq, dk, dv, None, None, None, None, None\n\n\n\n\ndef select_attn(q, k, v, select_size, fwd_ind, bwd_ind, sm_scale=None, inplace=True):\n    return _select_attention.apply(q, k, v, select_size, fwd_ind, bwd_ind, sm_scale, inplace)\n\n####################################################################################################################################################\n\nimport numpy as np\nimport random\nimport torch \nimport os\nfrom numpy.random import RandomState\nimport pytest\nfrom torch.testing import assert_close\nfrom geak_eval.perf.ROCm.performance_utils_pytest import PytestBenchmarker, do_bench_config, save_all_benchmark_results\nfrom typing import Dict\n\nimport triton\nimport triton.language as tl\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n    'bfloat16': torch.bfloat16,\n}\n\nresult_gold = {}\n\nclass NsaAttention(torch.nn.Module):\n    \"\"\"\n    native sparse attention.\n\n    Args:\n        qk_head_dim (int): head dim of q and k head\n\n        v_head_dim (int): head dim of v head\n\n        kernel_size (int): how many kv will be compressed and become a compressed kv block, the \"l\" in the paper\n\n        stride (int): like conv stride, compress the next block will move how many kv, the \"d\" in the paper\n\n        select_size (int): select block size, the \"l'\" in the paper\n\n        top_n (int): q will chosses how many select blocks.\n\n        window_size (int): sliding window size for window attention\n    \"\"\"\n    def __init__(self, qk_head_dim, v_head_dim, kernel_size=32, stride=16, select_size=64, top_n=16, window_size=512):\n        super().__init__()\n        self.qk_head_dim = qk_head_dim\n        self.v_head_dim = v_head_dim\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.select_size = select_size\n        self.top_n = top_n\n        self.window_size = window_size\n        self.sm_scale = qk_head_dim ** -0.5\n        assert math.log2(self.stride).is_integer()\n        assert kernel_size % stride == 0 and select_size % kernel_size == 0\n\n        self.compress_attn = CompressAttn(qk_head_dim, v_head_dim, kernel_size, stride)\n        self.select_for_fwd_bwd = partial(select_for_fwd_bwd, \n                                      kernel_size=self.kernel_size, \n                                      stride=self.stride, \n                                      select_size=self.select_size, \n                                      top_n=self.top_n, \n                                      sm_scale=self.sm_scale)\n        \n        self.select_attn = partial(select_attn, \n                                   select_size=self.select_size, \n                                   sm_scale=self.sm_scale)\n        \n        self.window_attn = partial(flash_attn_func, \n                                   softmax_scale=self.sm_scale, \n                                   causal=True, \n                                   window_size=(self.window_size, -1) )\n        self.attn_gate = torch.nn.Linear(qk_head_dim, 3)\n\n    \n    def forward(self, q, k, v, inplace=True):\n        \"\"\"\n        Forward pass for the NSA Attention module.\n\n        Args:\n            q (torch.Tensor): [b, seq_len, num_q_head, qk_head_dim]\n            k (torch.Tensor): [b, seq_len, num_kv_head, qk_head_dim]\n            v (torch.Tensor): [b, seq_len, num_kv_head, v_head_dim]\n            inplace (bool): in the backward the bwd_ind will be update in-place, set False for benchmark\n        Returns:\n            o (torch.Tensor): [b, seq_len, num_q_head, v_head_dim]\n        \"\"\"\n        cmp_o, lse, cmp_k = self.compress_attn(q, k, v)\n        _, fwd_ind, bwd_ind = self.select_for_fwd_bwd(q, cmp_k, lse)\n        select_o = self.select_attn(q, k, v, fwd_ind=fwd_ind, bwd_ind=bwd_ind, inplace=inplace)\n        window_o = self.window_attn(q, k, v)\n        if isinstance(window_o, Tuple):\n            window_o = window_o[0]\n        weight = self.attn_gate(q)\n        combine_o = fused_attention(cmp_o, select_o, window_o, weight)\n        return combine_o\n    \n######################################## HELPERS for Eval ######################################## \n# Helper function to define GB/s for NSA\ndef calculate_nsa_gbps(params: Dict, ms: float) -> float:\n    b = params['b']\n    n = params['n']\n    qh = params['qh']\n    kh = params['kh']\n    d = params['d']\n    vd = params['vd']\n    dtype = dtype_mapping[params['dtype_str']]\n    \n    # Calculate bytes for NSA operations\n    bytes_per_element = torch.tensor([], dtype=dtype).element_size()\n    # Read Q, K, V, write O\n    total_bytes = (b * n * qh * d +  # Q\n                   b * n * kh * d +   # K\n                   b * n * kh * vd +  # V\n                   b * n * qh * vd) * bytes_per_element  # O\n    \n    gbps = total_bytes / (ms / 1000) / 1e9\n    return gbps\n\n# Helper function to define TFLOPS for NSA\ndef calculate_nsa_tflops(params: Dict, ms: float) -> float:\n    b = params['b']\n    n = params['n']\n    qh = params['qh']\n    kh = params['kh']\n    d = params['d']\n    vd = params['vd']\n    \n    # Approximate FLOPs for attention operations\n    # QK^T: 2 * b * qh * n * n * d\n    # Softmax: ~5 * b * qh * n * n (approximation)\n    # Attention * V: 2 * b * qh * n * n * vd\n    flops = 2 * b * qh * n * n * d + 5 * b * qh * n * n + 2 * b * qh * n * n * vd\n    \n    tflops = flops / (ms / 1000) / 1e12\n    return tflops\n\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"\n    Set the random seed for reproducibility across multiple libraries and configure PyTorch for deterministic behavior.\n\n    Args:\n        seed (int): The seed value to set. Default is 42.\n    \"\"\"\n    # Set seed for Python's built-in random module\n    random.seed(seed)\n    # Set seed for NumPy\n    np.random.seed(seed)\n    # Set seed for PyTorch on CPU\n    torch.manual_seed(seed)\n    # Set seed for PyTorch on all GPUs (if available)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Ensure deterministic behavior in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set environment variable for hash-based operations\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n######################################## HELPERS for Eval ######################################## \n\n\nOP_NAME_FOR_BENCHMARK = \"nsa_attention_perf\"\n\n# (b, n, qh, kh, d, vd, stride, kernel_size, select_size, top_n, window_size, dtype_str)\n# Test parameters for NSA\ntest_params = [\n    (1, 4096, 64, 4, 128, 128, 16, 32, 64, 16, 512, 'bfloat16'),\n    (1, 8192, 64, 4, 128, 128, 16, 32, 64, 16, 512, 'bfloat16'),\n    (1, 16384, 64, 4, 128, 128, 16, 32, 64, 16, 512, 'bfloat16'),\n    (1, 32768, 64, 4, 128, 128, 16, 32, 64, 16, 512, 'bfloat16'),\n    (1, 65536, 64, 4, 128, 128, 16, 32, 64, 16, 512, 'bfloat16'),\n    (1, 131072, 64, 4, 128, 128, 16, 32, 64, 16, 512, 'bfloat16'),\n]\n@pytest.mark.parametrize('b,n,qh,kh,d,vd,stride,kernel_size,select_size,top_n,window_size,dtype_str', test_params)\ndef test_nsa(b, n, qh, kh, d, vd, stride, kernel_size, select_size, top_n, window_size, dtype_str, request):\n    set_seed()\n    \n    dtype = dtype_mapping[dtype_str]\n    device = 'cuda'\n    \n    # Create inputs\n    q = torch.randn(b, n, qh, d, device=device, dtype=dtype)\n    k = torch.randn(b, n, kh, d, device=device, dtype=dtype)\n    v = torch.randn(b, n, kh, vd, device=device, dtype=dtype)\n    \n    # Initialize NSA module\n    nsa = NsaAttention(d, vd, kernel_size, stride, select_size, top_n, window_size).to(device).to(dtype)\n    \n    # Forward pass\n    output = nsa(q, k, v, inplace=False)\n    \n    # torch.set_printoptions(profile='full')\n    \n    result_gold['_CALL_SUCCESS_'] = torch.tensor([[1.0]])\n    \n    ################### save tri_out in result_gold ###################\n    test_case_name = request.node.name\n    sanitized_key_name = test_case_name.replace(\"::\", \"_\").replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"-\", \"_\")\n    result_gold[sanitized_key_name] = output.clone().detach().cpu()\n    ################################################################### \n    \n    # Basic shape check\n    assert output.shape == (b, n, qh, vd)\n\n\n@pytest.mark.parametrize('b,n,qh,kh,d,vd,stride,kernel_size,select_size,top_n,window_size,dtype_str', test_params)\ndef test_performance(b, n, qh, kh, d, vd, stride, kernel_size, select_size, top_n, window_size, dtype_str, request):\n    set_seed()\n    \n    dtype = dtype_mapping[dtype_str]\n    device = 'cuda'\n    \n    # Create inputs\n    q = torch.randn(b, n, qh, d, device=device, dtype=dtype)\n    k = torch.randn(b, n, kh, d, device=device, dtype=dtype)\n    v = torch.randn(b, n, kh, vd, device=device, dtype=dtype)\n    \n    # Initialize NSA module\n    nsa = NsaAttention(d, vd, kernel_size, stride, select_size, top_n, window_size).to(device).to(dtype)\n    \n    # Operation lambda\n    op_lambda = lambda: nsa(q, k, v, inplace=False)\n    \n\n    bench_config = do_bench_config(\n        warm_up=10,      \n        repetition=50,   \n        quantiles=[0.5, 0.2, 0.8] \n    )\n    \n    benchmarker = PytestBenchmarker(op_callable=op_lambda,\n                                    op_name=OP_NAME_FOR_BENCHMARK,\n                                    config=bench_config)\n    \n    # Parameters for calculators\n    current_params_for_calculators = {\n        \"b\": b, \"n\": n, \"qh\": qh, \"kh\": kh, \"d\": d, \"vd\": vd,\n        \"stride\": stride, \"kernel_size\": kernel_size, \n        \"select_size\": select_size, \"top_n\": top_n,\n        \"window_size\": window_size, \"dtype_str\": dtype_str\n    }\n    \n    benchmarker.run_benchmark(current_params_dict=current_params_for_calculators,\n                              gbps_calculator=calculate_nsa_gbps,\n                              tflops_calculator=calculate_nsa_tflops)\n\n######################################## HELPERS for Eval ########################################     \n# --- Pytest hook to save the dictionary at the end of the session ---  \ndef test_save_results():  \n    \"\"\"  \n    Called after whole test run finished, right before returning the exit status to the system.  \n    \"\"\"\n    print('Inside session finish...')\n    if \"_CALL_SUCCESS_\" not in result_gold:\n        result_gold['_CALL_SUCCESS_'] = torch.tensor([[0.0]])\n    OUTPUT_FILENAME = __file__.replace('.','_') + '.pt'\n    print(f\"\\nSaving all y_triton results to {OUTPUT_FILENAME}...\")  \n    # Ensure the directory for the output file exists if it's in a subdirectory  \n    output_dir = os.path.dirname(OUTPUT_FILENAME)  \n    if output_dir and not os.path.exists(output_dir):  \n        os.makedirs(output_dir, exist_ok=True)  \n    torch.save(result_gold, OUTPUT_FILENAME)       \n    print(f\"Successfully saved {len(result_gold)} y_triton tensors to {OUTPUT_FILENAME}.\")  \n\n\ndef test_save_performance_results():\n    \"\"\"\n    Called after the test_performance function finishes.\n    This is a separate hook to ensure performance results are saved.\n    \"\"\"\n    print('\\nPytest session finishing... Saving benchmark results...')\n\n    output_directory = os.path.join(os.path.dirname(__file__), \"perf\")  # Save in a \"perf\" subdirectory next to the test file\n    os.makedirs(output_directory, exist_ok=True)\n    \n    save_all_benchmark_results(output_directory)\n    # import pdb; pdb.set_trace()\n    print(f\"All benchmark results attempted to save to: {output_directory}\")\n\n\n######################################## HELPERS for Eval ########################################\n\n"
}